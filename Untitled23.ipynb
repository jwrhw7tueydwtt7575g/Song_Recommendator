{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V2oV0iQhmk-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv('/songdata.csv')\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n"
      ],
      "metadata": {
        "id": "j7Do3nuZiGcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(n=5000).drop('link', axis=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "gBtrqqq5h08x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].str.lower().replace(r'[^\\w\\s]','').replace(r'\\n',' ', regex=True)\n",
        "df['text'][0]"
      ],
      "metadata": {
        "id": "HKCwbD4vh4Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def tokenization(txt):\n",
        "    tokens = nltk.word_tokenize(txt)\n",
        "    stemming = [stemmer.stem(w) for w in tokens]\n",
        "    return \" \".join(stemming)"
      ],
      "metadata": {
        "id": "MN6c20BKh8Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Download the 'punkt' and 'punkt_tab' resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the missing resource\n",
        "\n",
        "def tokenization(txt):\n",
        "    tokens = nltk.word_tokenize(txt)\n",
        "    stemming = [stemmer.stem(w) for w in tokens]\n",
        "    return \" \".join(stemming)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: tokenization(x))"
      ],
      "metadata": {
        "id": "DHgNmmugilpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "tfidvector = TfidfVectorizer(analyzer='word',stop_words='english')\n",
        "matrix = tfidvector.fit_transform(df['text'])\n",
        "similarity = cosine_similarity(matrix)"
      ],
      "metadata": {
        "id": "Gr5kUbK_i1yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "similarity[0]"
      ],
      "metadata": {
        "id": "RyHO5eA3i15o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommendation(song_df):\n",
        "    idx = df[df['song'] == song_df].index[0]\n",
        "    distances = sorted(list(enumerate(similarity[idx])),reverse=True,key=lambda x:x[1])\n",
        "\n",
        "    songs = []\n",
        "    for m_id in distances[1:21]:\n",
        "        songs.append(df.iloc[m_id[0]].song)\n",
        "\n",
        "    return songs\n",
        "recommendation('Alma Mater')"
      ],
      "metadata": {
        "id": "tos3P9ZVjGE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(similarity,open('similarity.pkl','wb'))\n",
        "pickle.dump(df,open('df.pkl','wb'))"
      ],
      "metadata": {
        "id": "GbbUyy6-jKwx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}